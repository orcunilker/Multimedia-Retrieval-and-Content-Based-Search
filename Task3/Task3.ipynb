{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task 3:&nbsp;Implementation of a Query Function over a Document Collection (30 Points)</h2>\n",
    "<p>Write a perform_query()&nbsp;function in Python that calculates the similarity between a query and a document vector on the basis of the Vector Space Model. The perform_query()&nbsp;function should rank the documents contained in a given document collection according to their score that is to be computed on the basis of the vector space model and the cosine similarity w.r.t. a previously issued query.</p>\n",
    "<p>For the implementation of this task, you can make use of the result/output produced by the&nbsp;calculate_tfidf()&nbsp;function as well as of the cosine similarity calculation algorithm implemented in Task 2.</p>\n",
    "<h3>Tasks:</h3>\n",
    "<ul>\n",
    "<li>Implement a perform_query()&nbsp;function that calculates the cosine similarity from a vector of vocabulary terms, a set of term frequency vectors, and a given query vector, where</li>\n",
    "<ul><li>the first parameter is the Pandas data frame, related to&nbsp;<b>tf value vectors</b></li>\n",
    "<li>the second parameter is a string containing the terms that constitute the issued query</li>\n",
    "</ul>\n",
    "<li>The function should return the calculated cosine similarity values in form of a ranked document list ordered by their relevance together with the query vector and the calculated tf values of the query terms with respect to the document collection against which it was issued.</li></ul>\n",
    "<ul>\n",
    "<li>\n",
    "<p>The signature of the perform_query() function expects two parameters. An exemplary invocation of the function is given below:</p>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document number</th>\n",
       "      <th>query document similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>0.771517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.745356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.654654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.640513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0.522233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  document number  query document similarity\n",
       "0               4                   0.771517\n",
       "0               1                   0.745356\n",
       "0               3                   0.654654\n",
       "0               2                   0.640513\n",
       "0               5                   0.522233"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%run Implementation.ipynb\n",
    "\n",
    "table_tfidf = calculate_tfidf('../collection1.txt')\n",
    "perform_query(table_tfidf, 'we pass the test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A possible output schema\n",
    "\n",
    "<table align=\"left\" border=\\1\\ class=\\dataframe\\>\n",
    "         <thead>\n",
    "           <tr style=\\text-align: center;\\>\n",
    "             <th>document number</th>\n",
    "             <th>query document similarity</th>\n",
    "           </tr>\n",
    "         </thead>\n",
    "         <tbody>\n",
    "           <tr>\n",
    "             <td>3</td>\n",
    "             <td>0.4321</td>\n",
    "           </tr>\n",
    "                        <tr>\n",
    "             <td>2</td>\n",
    "             <td>0.3214</td>\n",
    "           </tr>\n",
    "                        <tr>\n",
    "             <td>1</td>\n",
    "             <td>0.2143</td>\n",
    "           </tr>\n",
    "                        <tr>\n",
    "             <td>5</td>\n",
    "             <td>0.1432</td>\n",
    "           </tr>\n",
    "                        <tr>\n",
    "             <td>4</td>\n",
    "             <td>0.1324</td>\n",
    "           </tr>\n",
    "         </tbody>\n",
    "       </table>\n",
    "       \n",
    "       \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Test your implementation with a number of different queries and document collections (you can simply extend or modify the document collection&nbsp;collection1.txt)</li><li>Calculate the score based on the smart notation \"nnc.nnc\"</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Questions:\n",
    "#### 1. Think about the relationship between information need and query -- what is a query?\n",
    "A query does not neccessarly represent an information need. A query is a prompt given to the system. The information need is what the 'user' actually wants. The ability of the user to create a good query is decisive.\n",
    "\n",
    "#### 2. Critically think about the differences between the Boolean retrieval model and a ranked retrieval model building upon e.g. the vector space model.\n",
    "The boolean retrieval model is a rather primitive approach. It doesn't consider any weights or importances of terms. It solely contains information of whether it IS in a document OR NOT.\n",
    "The vector space model integrates weights, and thefore says a lot more about the relevance of a document given a specific query.\n",
    "\n",
    "#### 3. Analyze the behavior of the query function using the document collection collection2.txt\n",
    "##### 3.1 Explain why a query for the term \"a\" produces the given result\n",
    "(Doc)5 and 6 are ranked tge highest, as they solely contains a's.\n",
    "For 1, it contains a lot of a's, but it's not just all about a's anymore. There are a few other terms.\n",
    "And the less prominent 'a' gets in a document, the smaller the value gets.\n",
    "##### 3.2 Why receives Doc1 a higher rank than Doc3 when the term \"b\" is queried\n",
    "There are 1 b's in both But the occurance of 'b' in Doc1 plays a bigger role, as there are less terms in total.\n",
    "##### 3.3 Why is the rank of Doc3 lower than the rank of Doc2 when a query for the term \"c\" is issued?\n",
    "Because there are more c's for less terms.\n",
    "##### 3.4 Why receives Doc2 a higher rank than Doc4 when the query contains the terms \"b c\"?\n",
    "Because there are a lot more of both terms in Doc2.\n",
    "##### 3.5 Consider the query \"a b c d\" -- which document receives the highest rank and why?\n",
    "Firstly, because it contains all terms. Secondly, because doesn't contain any other term.\n",
    "#### 4. Critically think about what do these numbers represent and how do they express similarity?\n",
    "They represent the similarity of vectors in a space. For value 1, they are exactly the same. For 0, the vectors are normal to each other, no correspondance.\n",
    "Through the dot product, we can compute the similarity of a given document-vector and the query-vector - voilla, and we have our similarity measure.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
