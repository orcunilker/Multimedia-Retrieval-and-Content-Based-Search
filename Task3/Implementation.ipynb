{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                     # numpy\n",
    "import pandas as pd                    # pandas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"../Task2/Implementation.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateQueryEngine(dir: str):\n",
    "    featureMatrixDF = createFeatureMatrixForDirectory(dir=dir)\n",
    "    # normalize\n",
    "    for column in featureMatrixDF:\n",
    "        if column != 'File':\n",
    "            featureMatrixDF[column] = (featureMatrixDF[column] - featureMatrixDF[column].min()) / (featureMatrixDF[column].max() - featureMatrixDF[column].min())  \n",
    "    \n",
    "    featureMatrix = featureMatrixDF.to_numpy()\n",
    "    for mainFeatureVector in featureMatrix:\n",
    "        \n",
    "        evaluationFrame = pd.DataFrame(columns=['File', 'Relevance', 'Euclidean Distance', 'Precision', 'Recall', 'F'])\n",
    "        \n",
    "        # Euclidean Distance and adding to dataframe\n",
    "        for compareFeatureVector in featureMatrix:\n",
    "            if (mainFeatureVector!=compareFeatureVector).all(): #https://stackoverflow.com/questions/10580676/comparing-two-numpy-arrays-for-equality-element-wise #if vectors are not the same\n",
    "                A = mainFeatureVector[1:] # all dimensions except the filename\n",
    "                B = compareFeatureVector[1:]\n",
    "                euclideanDistance = np.linalg.norm(A-B)\n",
    "                newRow = pd.DataFrame.from_records([{ # add to frame\n",
    "                        'File': compareFeatureVector[0],\n",
    "                        'Euclidean Distance': euclideanDistance\n",
    "                    }])\n",
    "                evaluationFrame = pd.concat([evaluationFrame, newRow])\n",
    "        evaluationFrame = evaluationFrame.sort_values(by='Euclidean Distance', ascending=True)\n",
    "        evaluationFrame.reset_index(drop=True, inplace=True) # making indices standardly incrementing\n",
    "        \n",
    "        # Relevance\n",
    "        for fileName in evaluationFrame[\"File\"]: # detect relevance\n",
    "            if fileName[0:2] == mainFeatureVector[0][0:2]: # if they start with the same 2 characters\n",
    "                evaluationFrame.loc[evaluationFrame['File'] == fileName, 'Relevance'] = 1\n",
    "            else:\n",
    "                evaluationFrame.loc[evaluationFrame['File'] == fileName, 'Relevance'] = 0\n",
    "        \n",
    "        # Precision, Recall and F-Measure\n",
    "        retrievedCursor = 0\n",
    "        relevantCursor = 0\n",
    "        relevantItems = sum(evaluationFrame[\"Relevance\"])\n",
    "        for relevance in evaluationFrame[\"Relevance\"]:\n",
    "            retrievedCursor += 1\n",
    "            relevantCursor += relevance\n",
    "            \n",
    "            precision = relevantCursor/retrievedCursor\n",
    "            recall = relevantCursor/relevantItems\n",
    "            \n",
    "            evaluationFrame.loc[retrievedCursor - 1, 'Precision'] = precision # Set precision\n",
    "            evaluationFrame.loc[retrievedCursor - 1, 'Recall'] = recall # Set recall\n",
    "            \n",
    "            # f-measure\n",
    "            beta = 0.5\n",
    "            if precision > 0:\n",
    "                fmeasure = ((pow(beta, 2)+1) * precision * recall) / (pow(beta, 2)*precision + recall)\n",
    "            else:\n",
    "                fmeasure = 0\n",
    "            \n",
    "            evaluationFrame.loc[retrievedCursor - 1, 'F'] = fmeasure # Set f-measure\n",
    "            \n",
    "            \n",
    "        print()\n",
    "        print(mainFeatureVector[0])\n",
    "        display(evaluationFrame)\n",
    "        \n",
    "        print(\"Top 5:\")\n",
    "        print(\"Precision: \" + str(evaluationFrame['Precision'][4]))\n",
    "        print(\"Recall: \" + str(evaluationFrame['Recall'][4]))\n",
    "        print(\"F-Measure: \" + str(evaluationFrame['F'][4]))\n",
    "        print()\n",
    "        print(\"Top 10:\")\n",
    "        print(\"Precision: \" + str(evaluationFrame['Precision'][9]))\n",
    "        print(\"Recall: \" + str(evaluationFrame['Recall'][9]))\n",
    "        print(\"F-Measure: \" + str(evaluationFrame['F'][9]))\n",
    "        \n",
    "        plt.title(mainFeatureVector[0])\n",
    "        plt.plot(evaluationFrame['Recall'],evaluationFrame['Precision'])\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.xlim(0,1.02)\n",
    "        plt.ylim(0,1.02)\n",
    "        plt.show()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateQueryEngine(\"../videos/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
